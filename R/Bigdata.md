데이터 용량 커지면 한계가 있음
하둡과 알을 조합해서 쓰거나 or spark R
병렬처리와 ff bigmemory 등의 다른 패키지를 이용
또는 아에 새로운 언어 python이나 다른 언어를 큰 데이터 처리할때 쓰는거도 괜찮다고 생각

rpubs - handling large data set in R
https://rpubs.com/msundar/large_data_analysis
http://www.slideshare.net/bytemining/taking-r-to-the-limit-high-performance-computing-in-r-part-1-parallelization-la-r-users-group-727

https://www.r-bloggers.com/handling-large-datasets-in-r/

haddop+R rhipe
http://d2.naver.com/helloworld/1016
mongodb + hadoop


- ff packages 써보기

dataframe to ff
http://stackoverflow.com/questions/17251064/convert-data-frame-to-ff

http://www.dbguide.net/db.db?cmd=view&boardUid=187501&boardConfigUid=9&categoryUid=216&boardIdx=162&boardStep=1
